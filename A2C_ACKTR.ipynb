{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from multiprocessing import Process, Pipe\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "from torch.distributions import Normal, Categorical\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class Parser():\n",
    "    '''\n",
    "    Hyper-Parameters\n",
    "    '''\n",
    "    def __init__(self,):\n",
    "        self.lr = 7e-4\n",
    "        self.eps = 1e-5\n",
    "        self.alpha = 0.99\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.95\n",
    "        self.entropy_coef = 0.01\n",
    "        self.value_loss_coef = 0.5\n",
    "        self.seed = 1\n",
    "        self.num_processes = 16\n",
    "        self.num_steps = 5\n",
    "        self.num_frames = int(10e6)\n",
    "        self.env_name = \"Pendulum-v0\" # PongNoFrameskip-v4\n",
    "        self.model_path = './ACKTR/'\n",
    "args = Parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    '''\n",
    "    For plotting the rewards.\n",
    "    '''\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "\n",
    "def test_env(vis=False):\n",
    "    '''\n",
    "    Testing the model(actor_critic.base) during training.\n",
    "    '''\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            _,mu = actor_critic.base(state)\n",
    "            dist = Normal(mu, 1)\n",
    "            next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "            state = next_state\n",
    "            if vis: env.render()\n",
    "            total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SubprocVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For creating multiple envs\n",
    "'''\n",
    "\n",
    "def worker(remote, parent_remote, env_fn_wrapper):\n",
    "    parent_remote.close() \n",
    "    env = env_fn_wrapper()\n",
    "    while True: # Trying to read the data all the day? A:Yes\n",
    "        cmd, data = remote.recv()  # this remote is worker_remote\n",
    "        if cmd == 'step':\n",
    "            ob, reward, done, info = env.step(data)\n",
    "            if done:\n",
    "                ob = env.reset()\n",
    "            remote.send((ob, reward, done, info))\n",
    "        elif cmd == 'reset':\n",
    "            ob = env.reset()\n",
    "            remote.send(ob)\n",
    "        elif cmd == 'reset_task':\n",
    "            ob = env.reset_task()\n",
    "            remote.send(ob)\n",
    "        elif cmd == 'close':\n",
    "            remote.close()\n",
    "            break\n",
    "        elif cmd == 'get_spaces':\n",
    "            remote.send((env.observation_space, env.action_space))\n",
    "        elif cmd == 'Message':\n",
    "            print('Receive Message')\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "class SubprocVecEnv(object):\n",
    "    \n",
    "    def __init__(self, env_fns, spaces=None):\n",
    "        \"\"\"\n",
    "        envs: list of gym environments to run in subprocesses\n",
    "        \"\"\"\n",
    "        self.waiting = False\n",
    "        self.closed = False\n",
    "        nenvs = len(env_fns)\n",
    "        self.nenvs = nenvs\n",
    "        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n",
    "        self.ps = [Process(target=worker, args=(work_remote, remote, env_fn))\n",
    "            for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]\n",
    "        for p in self.ps:\n",
    "            p.daemon = True # if the main process crashes, we should not cause things to hang\n",
    "            p.start()\n",
    "        for remote in self.work_remotes: \n",
    "            remote.close() # Can be shut down after the start, and close remotes in the def(function doesn't affect the self.remots in this class)\n",
    "        self.remotes[0].send(('get_spaces', None))\n",
    "        self.observation_space, self.action_space = self.remotes[0].recv()\n",
    "        \n",
    "        \n",
    "    def step_async(self, actions):\n",
    "        for remote, action in zip(self.remotes, actions):\n",
    "            remote.send(('step', action)) # Send the message to worker remote\n",
    "        self.waiting = True\n",
    "\n",
    "    def step_wait(self):\n",
    "        results = [remote.recv() for remote in self.remotes]\n",
    "        self.waiting = False\n",
    "        obs, rews, dones, infos = zip(*results)\n",
    "        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n",
    "\n",
    "    def reset(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('reset', None))\n",
    "        return np.stack([remote.recv() for remote in self.remotes])\n",
    "\n",
    "    def reset_task(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('reset_task', None))\n",
    "        return np.stack([remote.recv() for remote in self.remotes])\n",
    "\n",
    "    def close(self):\n",
    "        if self.closed:\n",
    "            return\n",
    "        if self.waiting:\n",
    "            for remote in self.remotes:            \n",
    "                remote.recv()\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('close', None))\n",
    "        for p in self.ps:\n",
    "            p.join()\n",
    "            self.closed = True\n",
    "            \n",
    "    def step(self, actions):\n",
    "        self.step_async(actions)\n",
    "        return self.step_wait()\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.nenvs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For storing the collected \"Observation, rewards, values, returns, actions, action_log_probs, masks\"\n",
    "'''\n",
    "\n",
    "class RolloutStorage(object):\n",
    "\n",
    "    def __init__(self, num_steps, num_processes, obs_shape, action_space):\n",
    "        self.observations = torch.zeros(args.num_steps + 1, args.num_processes, *obs_shape).to(device) #(6,16,3)\n",
    "        self.rewards = torch.zeros(args.num_steps, args.num_processes, 1).to(device) # (5,16,1)\n",
    "        self.value_preds = torch.zeros(args.num_steps + 1, args.num_processes, 1).to(device) #(6,16,1)\n",
    "        self.returns = torch.zeros(args.num_steps + 1, args.num_processes, 1).to(device) #(6,16,1)\n",
    "        self.action_log_probs = torch.zeros(args.num_steps, args.num_processes, 1).to(device) #(6,16,1)\n",
    "        \n",
    "        if action_space.__class__.__name__ == 'Discrete':\n",
    "            action_shape = 1\n",
    "        else:\n",
    "            action_shape = action_space.shape[0]\n",
    "        self.actions = torch.zeros(args.num_steps, args.num_processes, action_shape).to(device) #(5,16,1)\n",
    "        if action_space.__class__.__name__ == 'Discrete':\n",
    "            self.actions = self.actions.long().to(device)\n",
    "            \n",
    "        self.masks = torch.ones(args.num_steps + 1, args.num_processes, 1).to(device) #(6,16,1)\n",
    "\n",
    "        self.num_steps = args.num_steps\n",
    "        self.step = 0\n",
    "\n",
    "    def insert(self, current_obs, action, action_log_prob, value_pred, reward, mask): # insert a new step\n",
    "        \n",
    "        self.observations[self.step + 1].copy_(current_obs) # insert the output to the self.\n",
    "        self.actions[self.step].copy_(action)\n",
    "        self.action_log_probs[self.step].copy_(action_log_prob)\n",
    "        self.value_preds[self.step].copy_(value_pred)\n",
    "        self.rewards[self.step].copy_(reward)\n",
    "        self.masks[self.step + 1].copy_(mask)\n",
    "\n",
    "        self.step = (self.step + 1) % self.num_steps # restrict the self.stpe in the self.num_steps\n",
    "\n",
    "    def after_update(self): # move the last one to the first and keep doing the next iter\n",
    "        self.observations[0].copy_(self.observations[-1]) # +1 in insert, these info will be used for the next iter\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "\n",
    "    def compute_returns(self, next_value, gamma, tau):\n",
    "        '''\n",
    "        Compute the returns by GAE, and it shouldn't require the gradient.\n",
    "        '''\n",
    "        self.value_preds[-1] = next_value\n",
    "        gae = 0\n",
    "        for step in reversed(range(self.rewards.size(0))):\n",
    "            delta = self.rewards[step] + gamma * self.value_preds[step + 1] * self.masks[step + 1] - self.value_preds[step]\n",
    "            gae = delta + gamma * tau * self.masks[step + 1] * gae\n",
    "            self.returns[step] = gae + self.value_preds[step]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, obs_shape, action_space, std=1):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        self.base = MLPBase(obs_shape[0]) # Use this model \n",
    "        \n",
    "        # Assign the distribution to different envs\n",
    "        if action_space.__class__.__name__ == \"Discrete\":\n",
    "            num_outputs = action_space.n\n",
    "            self.dist= Categorical\n",
    "        elif action_space.__class__.__name__ == \"Box\":\n",
    "            num_outputs = action_space.shape[0]\n",
    "            self.dist= Normal\n",
    "            self.std = std # The std will be set as 1 as default.\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, inputs):\n",
    "\n",
    "        '''\n",
    "        Only this method can generate a new action.\n",
    "        '''\n",
    "    \n",
    "        value, mu = self.base(inputs)\n",
    "        \n",
    "        dist = self.dist(mu ,self.std)\n",
    "\n",
    "        action = dist.sample()\n",
    "\n",
    "        action_log_probs = dist.log_prob(action)\n",
    "\n",
    "        return value, action, action_log_probs\n",
    "\n",
    "    def get_value(self, inputs):\n",
    "        '''\n",
    "        If we just need the actor_critic to return value.\n",
    "        '''\n",
    "        value, _= self.base(inputs) \n",
    "        return value\n",
    "\n",
    "    def evaluate_actions(self, inputs, action):\n",
    "        \n",
    "        '''\n",
    "        This will generate a similar result with actor_critic.act but this one usually requires the gradient.\n",
    "        The point of this method is to get the action_log_probs and dist_entropy(require gradient).\n",
    "        And the input of action contains the actions from all processes and steps.\n",
    "        '''\n",
    "        value, mu = self.base(inputs) \n",
    "        \n",
    "        dist = self.dist(mu, self.std)\n",
    "        \n",
    "        action_log_probs = dist.log_prob(action)\n",
    "        \n",
    "        dist_entropy = dist.entropy().mean()\n",
    "\n",
    "        return value, action_log_probs, dist_entropy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MLPBase(nn.Module):\n",
    "    def __init__(self, num_inputs, num_output = 1):\n",
    "        super(MLPBase, self).__init__()\n",
    "\n",
    "\n",
    "        self.actor = nn.Sequential(OrderedDict([\n",
    "            ('a_Linear1',nn.Linear(num_inputs, 64)),\n",
    "            ('a_Tanh1',nn.Tanh()),\n",
    "            ('a_Linear2',nn.Linear(64, 64)),\n",
    "            ('a_Tanh2',nn.Tanh()),\n",
    "            ('a_Linear3',nn.Linear(64,num_output)),\n",
    "        ]))\n",
    "\n",
    "        self.critic = nn.Sequential(OrderedDict([\n",
    "            ('c_Linear1',nn.Linear(num_inputs, 64)),\n",
    "            ('c_Tanh',nn.Tanh()),\n",
    "            ('c_Linear2',nn.Linear(64, 64)),\n",
    "            ('c_Tanh',nn.Tanh()),\n",
    "            ('c_Linear3',nn.Linear(64, 1)),\n",
    "        ]))\n",
    "\n",
    "        self.train()\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        value = self.critic(inputs)\n",
    "        mu = self.actor(inputs)\n",
    "\n",
    "        return value, mu\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor_critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C_ACKTR(object): \n",
    "    def __init__(self,\n",
    "                 actor_critic,\n",
    "                 acktr=False):\n",
    "\n",
    "        self.actor_critic = actor_critic # the policy NN will be sent to here\n",
    "        self.acktr = acktr\n",
    "\n",
    "        if acktr:\n",
    "            self.optimizer = KFACOptimizer(actor_critic) # the argument of lr can be added, the default lr is 0.25\n",
    "        else:\n",
    "            self.optimizer = optim.RMSprop(\n",
    "                actor_critic.parameters(), args.lr, eps=args.eps, alpha=args.alpha)\n",
    "\n",
    "    def update(self, rollouts):\n",
    "        \n",
    "        obs_shape = rollouts.observations.size()[2:] # (3)\n",
    "        action_shape = rollouts.actions.size()[-1] # (1)\n",
    "        num_steps, num_processes, _ = rollouts.rewards.size() # (5,16)\n",
    "\n",
    "        \n",
    "        # actor_critic.act was performed in the no_grad() condition, so we use the evaluate_action to get the gradient for updating the Policy NN\n",
    "        values, action_log_probs, dist_entropy = self.actor_critic.evaluate_actions(\n",
    "            rollouts.observations[:-1].view(-1, *obs_shape),\n",
    "            rollouts.actions.view(-1, action_shape)) # use old action to generate the prob, and use it as a single case\n",
    "        values = values.view(num_steps, num_processes, 1) # turn they back to the original shape\n",
    "        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n",
    "        \n",
    "        advantages = rollouts.returns[:-1] - values # The returns doesn't have gradient but the value has.\n",
    "        value_loss = advantages.pow(2).mean()\n",
    "\n",
    "        action_loss = -(advantages.detach() * action_log_probs).mean() # aciton_log_probs has the gradient\n",
    "\n",
    "        if self.acktr and self.optimizer.steps % self.optimizer.Ts == 0: # Ts = 1\n",
    "            self.actor_critic.zero_grad()\n",
    "            pg_fisher_loss = -action_log_probs.mean()\n",
    "            value_noise = torch.randn(values.size()).to(device)\n",
    "\n",
    "            sample_values = values + value_noise # add some noise\n",
    "            vf_fisher_loss = -(values - sample_values.detach()).pow(2).mean() # difference btw values/ values+noise\n",
    "            fisher_loss = pg_fisher_loss + vf_fisher_loss\n",
    "            self.optimizer.acc_stats = True # For updating m_gg <_save_grad_output mothod in K-FAC optimizer>\n",
    "            fisher_loss.backward(retain_graph=True)\n",
    "            self.optimizer.acc_stats = False\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        all_loss = value_loss * args.value_loss_coef + action_loss - dist_entropy * args.entropy_coef\n",
    "        all_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return all_loss, fisher_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFAC Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddBias(nn.Module):\n",
    "    def __init__(self, bias):\n",
    "        super(AddBias, self).__init__()\n",
    "        self._bias = nn.Parameter(bias.unsqueeze(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:\n",
    "            bias = self._bias.t().view(1, -1)\n",
    "        else:\n",
    "            bias = self._bias.t().view(1, -1, 1, 1)\n",
    "\n",
    "        return x + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_patches(x, kernel_size, stride, padding):\n",
    "    if padding[0] + padding[1] > 0:\n",
    "        x = F.pad(x, (padding[1], padding[1], padding[0],\n",
    "                      padding[0])).data  # Actually check dims\n",
    "    x = x.unfold(2, kernel_size[0], stride[0])\n",
    "    x = x.unfold(3, kernel_size[1], stride[1])\n",
    "    x = x.transpose_(1, 2).transpose_(2, 3).contiguous()\n",
    "    x = x.view(\n",
    "        x.size(0), x.size(1), x.size(2),\n",
    "        x.size(3) * x.size(4) * x.size(5))\n",
    "    return x\n",
    "\n",
    "\n",
    "def compute_cov_a(a, classname, layer_info, fast_cnn):\n",
    "    batch_size = a.size(0)\n",
    "\n",
    "    if classname == 'Conv2d':\n",
    "        if fast_cnn:\n",
    "            a = _extract_patches(a, *layer_info)\n",
    "            a = a.view(a.size(0), -1, a.size(-1))\n",
    "            a = a.mean(1)\n",
    "        else:\n",
    "            a = _extract_patches(a, *layer_info)\n",
    "            a = a.view(-1, a.size(-1)).div_(a.size(1)).div_(a.size(2))\n",
    "    elif classname == 'AddBias':\n",
    "        is_cuda = a.is_cuda\n",
    "        a = torch.ones(a.size(0), 1)\n",
    "        if is_cuda:\n",
    "            a = a.cuda()\n",
    "\n",
    "    return a.t() @ (a / batch_size)\n",
    "\n",
    "\n",
    "def compute_cov_g(g, classname, layer_info, fast_cnn):\n",
    "    batch_size = g.size(0)\n",
    "\n",
    "    if classname == 'Conv2d':\n",
    "        if fast_cnn:\n",
    "            g = g.view(g.size(0), g.size(1), -1)\n",
    "            g = g.sum(-1)\n",
    "        else:\n",
    "            g = g.transpose(1, 2).transpose(2, 3).contiguous()\n",
    "            g = g.view(-1, g.size(-1)).mul_(g.size(1)).mul_(g.size(2))\n",
    "    elif classname == 'AddBias':\n",
    "        g = g.view(g.size(0), g.size(1), -1)\n",
    "        g = g.sum(-1)\n",
    "\n",
    "    g_ = g * batch_size\n",
    "    return g_.t() @ (g_ / g.size(0))\n",
    "\n",
    "\n",
    "def update_running_stat(aa, m_aa, momentum):\n",
    "    # Do the trick to keep aa unchanged and not create any additional tensors\n",
    "    m_aa *= momentum / (1 - momentum)\n",
    "    m_aa += aa\n",
    "    m_aa *= (1 - momentum)\n",
    "\n",
    "\n",
    "class SplitBias(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(SplitBias, self).__init__()\n",
    "        self.module = module\n",
    "        self.add_bias = AddBias(module.bias.data)\n",
    "        self.module.bias = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.module(input)\n",
    "        x = self.add_bias(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class KFACOptimizer(optim.Optimizer):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 lr=0.25,\n",
    "                 momentum=0.9,\n",
    "                 stat_decay=0.99,\n",
    "                 kl_clip=0.001,\n",
    "                 damping=1e-2,\n",
    "                 weight_decay=0,\n",
    "                 fast_cnn=False,\n",
    "                 Ts=1,\n",
    "                 Tf=10):\n",
    "        defaults = dict()\n",
    "\n",
    "        def split_bias(module):\n",
    "            for mname, child in module.named_children():\n",
    "                if hasattr(child, 'bias') and child.bias is not None:\n",
    "                    module._modules[mname] = SplitBias(child)\n",
    "                else:\n",
    "                    split_bias(child)\n",
    "\n",
    "        split_bias(model)\n",
    "\n",
    "        super(KFACOptimizer, self).__init__(model.parameters(), defaults)\n",
    "\n",
    "        self.known_modules = {'Linear', 'Conv2d', 'AddBias'}\n",
    "\n",
    "        self.modules = []\n",
    "        self.grad_outputs = {}\n",
    "\n",
    "        self.model = model\n",
    "        self._prepare_model()\n",
    "\n",
    "        self.steps = 0\n",
    "\n",
    "        self.m_aa, self.m_gg = {}, {}\n",
    "        self.Q_a, self.Q_g = {}, {}\n",
    "        self.d_a, self.d_g = {}, {}\n",
    "\n",
    "        self.momentum = momentum\n",
    "        self.stat_decay = stat_decay\n",
    "\n",
    "        self.lr = lr\n",
    "        self.kl_clip = kl_clip\n",
    "        self.damping = damping\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.fast_cnn = fast_cnn\n",
    "\n",
    "        self.Ts = Ts #??\n",
    "        self.Tf = Tf\n",
    "\n",
    "        self.optim = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=self.lr * (1 - self.momentum),\n",
    "            momentum=self.momentum)\n",
    "\n",
    "    def _save_input(self, module, input):\n",
    "        if torch.is_grad_enabled() and self.steps % self.Ts == 0:\n",
    "            classname = module.__class__.__name__\n",
    "            layer_info = None\n",
    "            if classname == 'Conv2d':\n",
    "                layer_info = (module.kernel_size, module.stride,\n",
    "                              module.padding)\n",
    "\n",
    "            aa = compute_cov_a(input[0].data, classname, layer_info,\n",
    "                               self.fast_cnn)\n",
    "\n",
    "            # Initialize buffers\n",
    "            if self.steps == 0:\n",
    "                self.m_aa[module] = aa.clone()\n",
    "\n",
    "            update_running_stat(aa, self.m_aa[module], self.stat_decay)\n",
    "\n",
    "    def _save_grad_output(self, module, grad_input, grad_output):\n",
    "        if self.acc_stats:\n",
    "            classname = module.__class__.__name__\n",
    "            layer_info = None\n",
    "            if classname == 'Conv2d':\n",
    "                layer_info = (module.kernel_size, module.stride,\n",
    "                              module.padding)\n",
    "\n",
    "            gg = compute_cov_g(grad_output[0].data, classname, layer_info,\n",
    "                               self.fast_cnn)\n",
    "\n",
    "            # Initialize buffers\n",
    "            if self.steps == 0:\n",
    "                self.m_gg[module] = gg.clone()\n",
    "\n",
    "            update_running_stat(gg, self.m_gg[module], self.stat_decay)\n",
    "            \n",
    "\n",
    "    def _prepare_model(self):\n",
    "        for module in self.model.modules():\n",
    "            classname = module.__class__.__name__\n",
    "            if classname in self.known_modules:\n",
    "                assert not ((classname in ['Linear', 'Conv2d']) and module.bias is not None), \\\n",
    "                                    \"You must have a bias as a separate layer\"\n",
    "\n",
    "                self.modules.append(module)\n",
    "                module.register_forward_pre_hook(self._save_input)\n",
    "                module.register_backward_hook(self._save_grad_output)\n",
    "                \n",
    "\n",
    "    def step(self):\n",
    "        # Add weight decay\n",
    "        if self.weight_decay > 0:\n",
    "            for p in self.model.parameters():\n",
    "                p.grad.data.add_(self.weight_decay, p.data)\n",
    "\n",
    "        updates = {}\n",
    "        \n",
    "        for i, m in enumerate(self.modules):\n",
    "            assert len(list(m.parameters())\n",
    "                       ) == 1, \"Can handle only one parameter at the moment\"\n",
    "            classname = m.__class__.__name__\n",
    "            p = next(m.parameters())\n",
    "            \n",
    "            \n",
    "            la = self.damping + self.weight_decay\n",
    "            if self.steps % self.Tf == 0:\n",
    "                # My asynchronous implementation exists, I will add it later.\n",
    "                # Experimenting with different ways to this in PyTorch.\n",
    "                self.d_a[m], self.Q_a[m] = torch.symeig(\n",
    "                    self.m_aa[m], eigenvectors=True)\n",
    "                self.d_g[m], self.Q_g[m] = torch.symeig(\n",
    "                    self.m_gg[m], eigenvectors=True)\n",
    "\n",
    "                self.d_a[m].mul_((self.d_a[m] > 1e-6).float())\n",
    "                self.d_g[m].mul_((self.d_g[m] > 1e-6).float())\n",
    "\n",
    "            if classname == 'Conv2d':\n",
    "                p_grad_mat = p.grad.data.view(p.grad.data.size(0), -1)\n",
    "            else:\n",
    "                p_grad_mat = p.grad.data\n",
    "\n",
    "            v1 = self.Q_g[m].t() @ p_grad_mat @ self.Q_a[m]\n",
    "            v2 = v1 / (\n",
    "                self.d_g[m].unsqueeze(1) * self.d_a[m].unsqueeze(0) + la)\n",
    "            v = self.Q_g[m] @ v2 @ self.Q_a[m].t()\n",
    "\n",
    "            v = v.view(p.grad.data.size())\n",
    "            updates[p] = v\n",
    "\n",
    "        vg_sum = 0\n",
    "        for p in self.model.parameters():\n",
    "            v = updates[p]\n",
    "            vg_sum += (v * p.grad.data * self.lr * self.lr).sum()\n",
    "\n",
    "        nu = min(1, math.sqrt(self.kl_clip / vg_sum))\n",
    "\n",
    "        for p in self.model.parameters():\n",
    "            v = updates[p]\n",
    "            p.grad.data.copy_(v)\n",
    "            p.grad.data.mul_(nu)\n",
    "\n",
    "        self.optim.step()\n",
    "        self.steps += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-10:\n",
      "Process Process-14:\n",
      "Process Process-2:\n",
      "Process Process-16:\n",
      "Process Process-12:\n",
      "Process Process-11:\n",
      "Process Process-6:\n",
      "Process Process-3:\n",
      "Process Process-4:\n",
      "Process Process-1:\n",
      "Process Process-9:\n",
      "Process Process-7:\n",
      "Process Process-8:\n",
      "Process Process-15:\n",
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-13:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-4-ceb390e42b3d>\", line 9, in worker\n",
      "    cmd, data = remote.recv()  # this remote is worker_remote\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-ceb390e42b3d>\", line 9, in worker\n",
      "    cmd, data = remote.recv()  # this remote is worker_remote\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-ceb390e42b3d>\", line 9, in worker\n",
      "    cmd, data = remote.recv()  # this remote is worker_remote\n",
      "  File \"<ipython-input-4-ceb390e42b3d>\", line 9, in worker\n",
      "    cmd, data = remote.recv()  # this remote is worker_remote\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-ceb390e42b3d>\", line 9, in worker\n",
      "    cmd, data = remote.recv()  # this remote is worker_remote\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-ceb390e42b3d>\", line 9, in worker\n",
      "    cmd, data = remote.recv()  # this remote is worker_remote\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"<ipython-input-4-ceb390e42b3d>\", line 9, in worker\n",
      "    cmd, data = remote.recv()  # this remote is worker_remote\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"<ipython-input-4-ceb390e42b3d>\", line 9, in worker\n",
      "    cmd, data = remote.recv()  # this remote is worker_remote\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-ceb390e42b3d>\", line 9, in worker\n",
      "    cmd, data = remote.recv()  # this remote is worker_remote\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-4-ceb390e42b3d>\", line 9, in worker\n",
      "    cmd, data = remote.recv()  # this remote is worker_remote\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"<ipython-input-4-ceb390e42b3d>\", line 9, in worker\n",
      "    cmd, data = remote.recv()  # this remote is worker_remote\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"<ipython-input-4-ceb390e42b3d>\", line 9, in worker\n",
      "    cmd, data = remote.recv()  # this remote is worker_remote\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-4-ceb390e42b3d>\", line 9, in worker\n",
      "    cmd, data = remote.recv()  # this remote is worker_remote\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-4-ceb390e42b3d>\", line 9, in worker\n",
      "    cmd, data = remote.recv()  # this remote is worker_remote\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-ceb390e42b3d>\", line 9, in worker\n",
      "    cmd, data = remote.recv()  # this remote is worker_remote\n",
      "  File \"<ipython-input-4-ceb390e42b3d>\", line 9, in worker\n",
      "    cmd, data = remote.recv()  # this remote is worker_remote\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1a3d16fdf85c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_returns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Compute the returns before the update, then in the update function, the advantage function can be accessible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mallL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfisherL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-fda4a739d74d>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, rollouts)\u001b[0m\n\u001b[1;32m     23\u001b[0m         values, action_log_probs, dist_entropy = self.actor_critic.evaluate_actions(\n\u001b[1;32m     24\u001b[0m             \u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mobs_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             rollouts.actions.view(-1, action_shape)) # use old action to generate the prob, and use it as a single case\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_processes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# turn they back to the original shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0maction_log_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_log_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_processes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d37f00250f86>\u001b[0m in \u001b[0;36mevaluate_actions\u001b[0;34m(self, inputs, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mAnd\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mof\u001b[0m \u001b[0maction\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mactions\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mall\u001b[0m \u001b[0mprocesses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         '''\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d37f00250f86>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-824925cf9f14>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-55a53e9aa56a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if not os.path.exists(args.model_path):\n",
    "    os.makedirs(args.model_path)\n",
    "    \n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(args.env_name)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "env = gym.make(args.env_name) # made for testing\n",
    "envs = [make_env() for i in range(args.num_processes)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "actor_critic = Policy(envs.observation_space.shape, envs.action_space)\n",
    "\n",
    "agent = A2C_ACKTR(actor_critic, acktr=True)\n",
    "\n",
    "obs_shape = envs.observation_space.shape\n",
    "\n",
    "rollouts = RolloutStorage(args.num_steps, args.num_processes, obs_shape, envs.action_space)\n",
    "\n",
    "current_obs = torch.zeros(args.num_processes, *obs_shape)\n",
    "\n",
    "obs = envs.reset()\n",
    "\n",
    "current_obs = torch.tensor(obs).float()\n",
    "rollouts.observations[0].copy_(current_obs)\n",
    "\n",
    "test_rewards=[]\n",
    "\n",
    "for j in range(args.num_frames):\n",
    "    for step in range(args.num_steps):\n",
    "        # Sample actions\n",
    "        with torch.no_grad():\n",
    "\n",
    "            value, action, action_log_prob = actor_critic.act(\n",
    "                    rollouts.observations[step])\n",
    "\n",
    "        cpu_action = action.cpu().numpy() # turn the action to numpy, which can be sent to interact with envs.\n",
    "        obs, reward, done,_ = envs.step(cpu_action)\n",
    "        \n",
    "        reward = torch.tensor(reward).unsqueeze(1).float() \n",
    "        \n",
    "        masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done]) \n",
    "        \n",
    "        current_obs *= masks \n",
    "        current_obs = torch.tensor(obs).float()\n",
    "        rollouts.insert(current_obs,action, action_log_prob, value, reward, masks) # Insert the collected data to rollout\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        next_value = actor_critic.get_value(rollouts.observations[-1]).detach() # Get next_value\n",
    "\n",
    "    rollouts.compute_returns(next_value, args.gamma, args.tau) # Compute the returns before the update, then in the update function, the advantage function can be accessible.\n",
    "    allL, fisherL = agent.update(rollouts) # Optimizing the model\n",
    "    rollouts.after_update() # Prepare for next iter\n",
    "    \n",
    "    \n",
    "    if j % 1000 == 0:\n",
    "        test_rewards.append(np.mean([test_env() for _ in range(10)]))\n",
    "        plot(j, test_rewards)\n",
    "\n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
