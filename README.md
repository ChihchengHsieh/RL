# RL
[[OpenAI Baselines](https://github.com/openai/baselines)]

## DQL
[[DQL Paper](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)]
[[Code from](https://github.com/higgsfield/RL-Adventure)]
[[Official Pytorch Tut](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)]
## PPO
[[PPO Paper](https://arxiv.org/pdf/1707.06347.pdf)]
[[OpenAI Blog](https://blog.openai.com/openai-baselines-ppo/)]
[[Code from](https://github.com/higgsfield/RL-Adventure-2)]

## ACKTR
[[Paper](https://arxiv.org/pdf/1708.05144.pdf)]
[[Code from](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr)]
[[K-FAC Optimizer Paper](https://arxiv.org/pdf/1503.05671.pdf)]
[[Nice Interpretation](https://medium.com/@yaroslavvb/optimizing-deeper-networks-with-kfac-in-pytorch-4004adcba1b0)]
[[OpenAI Blog](https://blog.openai.com/baselines-acktr-a2c/)]

ACKTR combines three distinct techniques: actor-critic methods, trust region optimization for more consistent improvement, and distributed Kronecker factorization to improve sample efficiency and scalability.
